{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ba75db-3433-461a-86e3-06fa1f8e005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "import lovely_tensors\n",
    "lovely_tensors.monkey_patch()\n",
    "\n",
    "from vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb977ee-2c06-44ac-835a-88f7b326e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 10] x∈[-0.584, 0.611] μ=-0.083 σ=0.406 grad AddmmBackward0 [[-0.445, 0.457, -0.584, -0.556, -0.276, 0.100, 0.101, -0.105, -0.137, 0.611]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViT(\n",
    "    img_size=32, \n",
    "    patch_size=4,    \n",
    "    in_channels=3, \n",
    "    embed_size=256, \n",
    "    num_heads=8, \n",
    "    depth=6,\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "out = vit(input_tensor)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74806d86-6126-4575-b4d9-4bb1147a8045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3922c5-7197-4210-bb7b-60362188198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwoj-jasinski\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wojtek/mro/vit/wandb/run-20231129_184019-18hlo7vz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/woj-jasinski/MRO-VIT/runs/18hlo7vz' target=\"_blank\">mild-leaf-10</a></strong> to <a href='https://wandb.ai/woj-jasinski/MRO-VIT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/woj-jasinski/MRO-VIT' target=\"_blank\">https://wandb.ai/woj-jasinski/MRO-VIT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/woj-jasinski/MRO-VIT/runs/18hlo7vz' target=\"_blank\">https://wandb.ai/woj-jasinski/MRO-VIT/runs/18hlo7vz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6f58dba60e4ecfb7bc7b98e25fbae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, lr: 0.002, loss/train: 1.8000186492719918, loss/test: 1.5120775171473055, acc/train: 0.3376, acc/test: 0.4495\n",
      "epoch: 1, lr: 0.002, loss/train: 1.4737884120258224, loss/test: 1.3767229635504228, acc/train: 0.46202, acc/test: 0.5016\n",
      "epoch: 2, lr: 0.002, loss/train: 1.3660748270161622, loss/test: 1.3069208920756472, acc/train: 0.50728, acc/test: 0.5294\n",
      "epoch: 3, lr: 0.002, loss/train: 1.3007862763026792, loss/test: 1.2506943111178241, acc/train: 0.53264, acc/test: 0.5469\n",
      "epoch: 4, lr: 0.002, loss/train: 1.2493867419869698, loss/test: 1.2306218207636965, acc/train: 0.55136, acc/test: 0.5581\n",
      "epoch: 5, lr: 0.002, loss/train: 1.200251328060999, loss/test: 1.1777015237868587, acc/train: 0.56978, acc/test: 0.5824\n",
      "epoch: 6, lr: 0.002, loss/train: 1.1635121986689165, loss/test: 1.1462715366218663, acc/train: 0.5854, acc/test: 0.5878\n",
      "epoch: 7, lr: 0.002, loss/train: 1.133148251896929, loss/test: 1.116226367558105, acc/train: 0.59752, acc/test: 0.6015\n",
      "epoch: 8, lr: 0.002, loss/train: 1.1162657138636656, loss/test: 1.1139377325395994, acc/train: 0.60138, acc/test: 0.6008\n",
      "epoch: 9, lr: 0.002, loss/train: 1.0803429848702668, loss/test: 1.0975562982921359, acc/train: 0.6169, acc/test: 0.6087\n",
      "epoch: 10, lr: 0.002, loss/train: 1.0493624751525157, loss/test: 1.0329319140579127, acc/train: 0.62808, acc/test: 0.6302\n",
      "epoch: 11, lr: 0.002, loss/train: 1.0189032563772957, loss/test: 1.0161734140371974, acc/train: 0.63868, acc/test: 0.6403\n",
      "epoch: 12, lr: 0.002, loss/train: 0.9985011407481436, loss/test: 1.0197213005416002, acc/train: 0.64524, acc/test: 0.6384\n",
      "epoch: 13, lr: 0.002, loss/train: 0.9793157357998821, loss/test: 0.9927301459674593, acc/train: 0.65432, acc/test: 0.6494\n",
      "epoch: 14, lr: 0.002, loss/train: 0.9604214077715374, loss/test: 0.9859633038315592, acc/train: 0.6616, acc/test: 0.6531\n",
      "epoch: 15, lr: 0.002, loss/train: 0.9457684898620371, loss/test: 0.9724220346800888, acc/train: 0.66628, acc/test: 0.6597\n",
      "epoch: 16, lr: 0.002, loss/train: 0.9342823191676908, loss/test: 0.9580795908276039, acc/train: 0.67112, acc/test: 0.6578\n",
      "epoch: 17, lr: 0.002, loss/train: 0.9260166522182162, loss/test: 0.9600965554201151, acc/train: 0.67574, acc/test: 0.6636\n",
      "epoch: 18, lr: 0.002, loss/train: 0.9147608862508594, loss/test: 0.9363207726538936, acc/train: 0.67948, acc/test: 0.6725\n",
      "epoch: 19, lr: 0.002, loss/train: 0.9036939877378362, loss/test: 0.9712253529814225, acc/train: 0.67992, acc/test: 0.6569\n",
      "epoch: 20, lr: 0.002, loss/train: 0.8950921435795172, loss/test: 0.9272123763832865, acc/train: 0.6852, acc/test: 0.6758\n",
      "epoch: 21, lr: 0.002, loss/train: 0.881222531466228, loss/test: 0.9202279871023153, acc/train: 0.69048, acc/test: 0.6718\n",
      "epoch: 22, lr: 0.002, loss/train: 0.8747919435086458, loss/test: 0.9052412758899641, acc/train: 0.6911, acc/test: 0.6826\n",
      "epoch: 23, lr: 0.002, loss/train: 0.8745619459530277, loss/test: 0.9086565133891528, acc/train: 0.69376, acc/test: 0.6774\n",
      "epoch: 24, lr: 0.002, loss/train: 0.86624066908951, loss/test: 0.9167879442625408, acc/train: 0.69676, acc/test: 0.6758\n",
      "epoch: 25, lr: 0.002, loss/train: 0.8597790350389602, loss/test: 0.8822965682307377, acc/train: 0.69624, acc/test: 0.6877\n",
      "epoch: 26, lr: 0.002, loss/train: 0.8513382778448217, loss/test: 0.9159077232397055, acc/train: 0.70072, acc/test: 0.6792\n",
      "epoch: 27, lr: 0.002, loss/train: 0.8464457110675705, loss/test: 0.9000801564771918, acc/train: 0.70174, acc/test: 0.6865\n",
      "epoch: 28, lr: 0.002, loss/train: 0.8398198130185647, loss/test: 0.8824730613563634, acc/train: 0.70414, acc/test: 0.6873\n",
      "epoch: 29, lr: 0.002, loss/train: 0.8325683255024883, loss/test: 0.8926074867007099, acc/train: 0.70626, acc/test: 0.6843\n",
      "epoch: 30, lr: 0.002, loss/train: 0.8320465601618637, loss/test: 0.8970058092588111, acc/train: 0.70734, acc/test: 0.6894\n",
      "epoch: 31, lr: 0.002, loss/train: 0.8223953003163837, loss/test: 0.8881729901591434, acc/train: 0.70932, acc/test: 0.6927\n",
      "epoch: 32, lr: 0.002, loss/train: 0.8227166053279281, loss/test: 0.8731178799762002, acc/train: 0.71048, acc/test: 0.6991\n",
      "epoch: 33, lr: 0.002, loss/train: 0.8140170225097091, loss/test: 0.9002824040907847, acc/train: 0.71464, acc/test: 0.6875\n",
      "epoch: 34, lr: 0.002, loss/train: 0.8109881353500249, loss/test: 0.8744771759721297, acc/train: 0.71682, acc/test: 0.6959\n",
      "epoch: 35, lr: 0.002, loss/train: 0.8061890153933668, loss/test: 0.8830726697475095, acc/train: 0.71772, acc/test: 0.6946\n",
      "epoch: 36, lr: 0.002, loss/train: 0.7975484958999907, loss/test: 0.899002626726899, acc/train: 0.72064, acc/test: 0.6979\n",
      "epoch: 37, lr: 0.002, loss/train: 0.7964408952561791, loss/test: 0.863891134533701, acc/train: 0.72092, acc/test: 0.7022\n",
      "epoch: 38, lr: 0.002, loss/train: 0.7912451412214343, loss/test: 0.8751860276053224, acc/train: 0.72276, acc/test: 0.701\n",
      "epoch: 39, lr: 0.002, loss/train: 0.7804752571503525, loss/test: 0.8394142389297485, acc/train: 0.72608, acc/test: 0.7111\n",
      "epoch: 40, lr: 0.002, loss/train: 0.7826145768470472, loss/test: 0.8346315509156336, acc/train: 0.72504, acc/test: 0.7092\n",
      "epoch: 41, lr: 0.002, loss/train: 0.7779782568402303, loss/test: 0.8564902702464333, acc/train: 0.7251, acc/test: 0.7051\n",
      "epoch: 42, lr: 0.002, loss/train: 0.7748815635281145, loss/test: 0.851922207995306, acc/train: 0.72586, acc/test: 0.7047\n",
      "epoch: 43, lr: 0.002, loss/train: 0.770426039348173, loss/test: 0.8294906005074706, acc/train: 0.7285, acc/test: 0.7126\n",
      "epoch: 44, lr: 0.002, loss/train: 0.7676269201671376, loss/test: 0.8295027568370481, acc/train: 0.73018, acc/test: 0.7067\n",
      "epoch: 45, lr: 0.002, loss/train: 0.7604357485880937, loss/test: 0.8367820321759091, acc/train: 0.73432, acc/test: 0.7116\n",
      "epoch: 46, lr: 0.002, loss/train: 0.7540011029414204, loss/test: 0.8703373715847353, acc/train: 0.73698, acc/test: 0.6978\n",
      "epoch: 47, lr: 0.002, loss/train: 0.7566520205848967, loss/test: 0.8288802059390877, acc/train: 0.73352, acc/test: 0.7155\n",
      "epoch: 48, lr: 0.002, loss/train: 0.7435626153598356, loss/test: 0.8221766714808307, acc/train: 0.73876, acc/test: 0.7101\n",
      "epoch: 49, lr: 0.002, loss/train: 0.7439715137414615, loss/test: 0.8212954673586013, acc/train: 0.74004, acc/test: 0.7153\n",
      "epoch: 50, lr: 0.002, loss/train: 0.7397843601606081, loss/test: 0.834658492215072, acc/train: 0.73996, acc/test: 0.7129\n",
      "epoch: 51, lr: 0.002, loss/train: 0.7327080328598656, loss/test: 0.8165046421787406, acc/train: 0.74358, acc/test: 0.7163\n",
      "epoch: 52, lr: 0.002, loss/train: 0.7338704102484467, loss/test: 0.8099055290222168, acc/train: 0.74258, acc/test: 0.7113\n",
      "epoch: 53, lr: 0.002, loss/train: 0.7234492428467402, loss/test: 0.8126452980162222, acc/train: 0.74576, acc/test: 0.718\n",
      "epoch: 54, lr: 0.002, loss/train: 0.7229356628549678, loss/test: 0.808900976482826, acc/train: 0.74446, acc/test: 0.7225\n",
      "epoch: 55, lr: 0.002, loss/train: 0.7176626113522083, loss/test: 0.8071648403059078, acc/train: 0.74968, acc/test: 0.7228\n",
      "epoch: 56, lr: 0.002, loss/train: 0.7115863851269187, loss/test: 0.8099465392812898, acc/train: 0.74988, acc/test: 0.7197\n",
      "epoch: 57, lr: 0.002, loss/train: 0.7068799231058497, loss/test: 0.7974882125854492, acc/train: 0.7501, acc/test: 0.7237\n",
      "epoch: 58, lr: 0.002, loss/train: 0.7066414399677531, loss/test: 0.8085649949085864, acc/train: 0.751, acc/test: 0.7226\n",
      "epoch: 59, lr: 0.002, loss/train: 0.7010861206847383, loss/test: 0.7920608844938157, acc/train: 0.75234, acc/test: 0.7267\n",
      "epoch: 60, lr: 0.002, loss/train: 0.6943282677084589, loss/test: 0.7772557463827012, acc/train: 0.75472, acc/test: 0.7331\n",
      "epoch: 61, lr: 0.002, loss/train: 0.6955378380273004, loss/test: 0.7879902275302743, acc/train: 0.75692, acc/test: 0.7317\n",
      "epoch: 62, lr: 0.002, loss/train: 0.6941087552348671, loss/test: 0.792669869676421, acc/train: 0.7548, acc/test: 0.7263\n",
      "epoch: 63, lr: 0.002, loss/train: 0.6861120398392153, loss/test: 0.8025991230071345, acc/train: 0.7597, acc/test: 0.7299\n",
      "epoch: 64, lr: 0.002, loss/train: 0.686359048042151, loss/test: 0.7808505274072478, acc/train: 0.75818, acc/test: 0.7288\n",
      "epoch: 65, lr: 0.002, loss/train: 0.6856351925436494, loss/test: 0.7910993144481997, acc/train: 0.76086, acc/test: 0.7283\n",
      "epoch: 66, lr: 0.002, loss/train: 0.6830734019846563, loss/test: 0.8033335465419141, acc/train: 0.76076, acc/test: 0.7301\n",
      "epoch: 67, lr: 0.002, loss/train: 0.6777901929205336, loss/test: 0.8094826008700118, acc/train: 0.7616, acc/test: 0.7262\n",
      "epoch: 68, lr: 0.002, loss/train: 0.6823552125860053, loss/test: 0.7994833814946911, acc/train: 0.759, acc/test: 0.7286\n",
      "epoch: 69, lr: 0.002, loss/train: 0.6764198522586042, loss/test: 0.7874685925773427, acc/train: 0.7611, acc/test: 0.7262\n",
      "epoch: 70, lr: 0.002, loss/train: 0.6704200634261226, loss/test: 0.8061790896367423, acc/train: 0.76376, acc/test: 0.7243\n",
      "epoch: 71, lr: 0.002, loss/train: 0.6722460218402736, loss/test: 0.7819014757494384, acc/train: 0.76318, acc/test: 0.7335\n",
      "epoch: 72, lr: 0.002, loss/train: 0.6645402872501431, loss/test: 0.7777529856826686, acc/train: 0.76486, acc/test: 0.7355\n",
      "epoch: 73, lr: 0.002, loss/train: 0.6594039329025142, loss/test: 0.7847635976121395, acc/train: 0.76852, acc/test: 0.7286\n",
      "epoch: 74, lr: 0.002, loss/train: 0.6617356238462736, loss/test: 0.7764959629577927, acc/train: 0.76872, acc/test: 0.7346\n",
      "epoch: 75, lr: 0.002, loss/train: 0.6602913492628376, loss/test: 0.7758105970636199, acc/train: 0.76842, acc/test: 0.7364\n",
      "epoch: 76, lr: 0.002, loss/train: 0.6576953848914417, loss/test: 0.7705265185501002, acc/train: 0.76712, acc/test: 0.7373\n",
      "epoch: 77, lr: 0.002, loss/train: 0.6571862473512244, loss/test: 0.7941519633124147, acc/train: 0.76862, acc/test: 0.7309\n",
      "epoch: 78, lr: 0.002, loss/train: 0.6573949883051236, loss/test: 0.7674957785425307, acc/train: 0.76834, acc/test: 0.7369\n",
      "epoch: 79, lr: 0.002, loss/train: 0.6482269225065665, loss/test: 0.7686706343783608, acc/train: 0.77066, acc/test: 0.735\n",
      "epoch: 80, lr: 0.002, loss/train: 0.6508041332902201, loss/test: 0.8040290663513956, acc/train: 0.77212, acc/test: 0.7273\n",
      "epoch: 81, lr: 0.002, loss/train: 0.6466073831328956, loss/test: 0.7668732193451894, acc/train: 0.77362, acc/test: 0.7337\n",
      "epoch: 82, lr: 0.002, loss/train: 0.6451631696785197, loss/test: 0.7517491307439683, acc/train: 0.77368, acc/test: 0.7478\n",
      "epoch: 83, lr: 0.002, loss/train: 0.6404285177855236, loss/test: 0.7870421681223037, acc/train: 0.77472, acc/test: 0.7292\n",
      "epoch: 84, lr: 0.002, loss/train: 0.6366792923349249, loss/test: 0.7803618998467168, acc/train: 0.77586, acc/test: 0.7382\n",
      "epoch: 85, lr: 0.002, loss/train: 0.6363661118480556, loss/test: 0.7698362782786164, acc/train: 0.77536, acc/test: 0.7368\n",
      "epoch: 86, lr: 0.002, loss/train: 0.6309132355710735, loss/test: 0.747400047658365, acc/train: 0.778, acc/test: 0.7399\n",
      "epoch: 87, lr: 0.002, loss/train: 0.6283608706253568, loss/test: 0.7450054567071456, acc/train: 0.77798, acc/test: 0.7474\n",
      "epoch: 88, lr: 0.002, loss/train: 0.6268598642343145, loss/test: 0.7399604712860494, acc/train: 0.78098, acc/test: 0.7488\n",
      "epoch: 89, lr: 0.002, loss/train: 0.6270324711299613, loss/test: 0.7764318494857112, acc/train: 0.77752, acc/test: 0.7381\n",
      "epoch: 90, lr: 0.002, loss/train: 0.616330314551473, loss/test: 0.7489781741854511, acc/train: 0.78218, acc/test: 0.7436\n",
      "epoch: 91, lr: 0.002, loss/train: 0.6195295171054733, loss/test: 0.7480147465874877, acc/train: 0.78308, acc/test: 0.7475\n",
      "epoch: 92, lr: 0.002, loss/train: 0.6180488350598708, loss/test: 0.7488246311115313, acc/train: 0.7831, acc/test: 0.7473\n",
      "epoch: 93, lr: 0.002, loss/train: 0.6149057226870066, loss/test: 0.7539692087264, acc/train: 0.78272, acc/test: 0.7472\n",
      "epoch: 94, lr: 0.002, loss/train: 0.6129472247322502, loss/test: 0.7416160853603219, acc/train: 0.7816, acc/test: 0.75\n",
      "epoch: 95, lr: 0.002, loss/train: 0.6113189412352374, loss/test: 0.7658007333550272, acc/train: 0.78566, acc/test: 0.7444\n",
      "epoch: 96, lr: 0.002, loss/train: 0.6052283925168654, loss/test: 0.7557876706123352, acc/train: 0.78818, acc/test: 0.742\n",
      "epoch: 97, lr: 0.002, loss/train: 0.6097785912053969, loss/test: 0.7721949091440514, acc/train: 0.78518, acc/test: 0.7346\n",
      "epoch: 98, lr: 0.002, loss/train: 0.6070718455040242, loss/test: 0.780860233155987, acc/train: 0.78502, acc/test: 0.7438\n",
      "epoch: 99, lr: 0.002, loss/train: 0.5994975342012733, loss/test: 0.7454975617082813, acc/train: 0.78918, acc/test: 0.7491\n",
      "LR drop\n",
      "epoch: 100, lr: 0.0002, loss/train: 0.6034272243757077, loss/test: 0.7709405060055889, acc/train: 0.78578, acc/test: 0.74\n",
      "epoch: 101, lr: 0.0002, loss/train: 0.5208467955479537, loss/test: 0.6805068326147297, acc/train: 0.81566, acc/test: 0.7714\n",
      "epoch: 102, lr: 0.0002, loss/train: 0.48603444201562107, loss/test: 0.685430185327047, acc/train: 0.82986, acc/test: 0.7679\n",
      "epoch: 103, lr: 0.0002, loss/train: 0.48230085470487394, loss/test: 0.6657865866075589, acc/train: 0.82996, acc/test: 0.7788\n",
      "epoch: 104, lr: 0.0002, loss/train: 0.4709686611009681, loss/test: 0.6862963951086696, acc/train: 0.83466, acc/test: 0.7731\n",
      "epoch: 105, lr: 0.0002, loss/train: 0.4710890963254377, loss/test: 0.6783155353763436, acc/train: 0.8349, acc/test: 0.7722\n",
      "epoch: 106, lr: 0.0002, loss/train: 0.46411238302050345, loss/test: 0.684851612471327, acc/train: 0.83504, acc/test: 0.7734\n",
      "epoch: 107, lr: 0.0002, loss/train: 0.4635059310652106, loss/test: 0.6850423099873941, acc/train: 0.83804, acc/test: 0.7729\n",
      "epoch: 108, lr: 0.0002, loss/train: 0.45792610116322024, loss/test: 0.6820211501061162, acc/train: 0.84038, acc/test: 0.772\n",
      "epoch: 109, lr: 0.0002, loss/train: 0.45520413417340544, loss/test: 0.6736546302143531, acc/train: 0.83936, acc/test: 0.7754\n",
      "epoch: 110, lr: 0.0002, loss/train: 0.4516309470014499, loss/test: 0.6847129739538024, acc/train: 0.84014, acc/test: 0.7745\n",
      "epoch: 111, lr: 0.0002, loss/train: 0.44787569744202793, loss/test: 0.6842198202127143, acc/train: 0.84434, acc/test: 0.7773\n",
      "epoch: 112, lr: 0.0002, loss/train: 0.4438459060304915, loss/test: 0.6839101820052425, acc/train: 0.8427, acc/test: 0.7747\n",
      "epoch: 113, lr: 0.0002, loss/train: 0.44599416909162953, loss/test: 0.7012314905848684, acc/train: 0.84204, acc/test: 0.7713\n",
      "epoch: 114, lr: 0.0002, loss/train: 0.44275978863086846, loss/test: 0.6884303304213512, acc/train: 0.84506, acc/test: 0.7758\n",
      "epoch: 115, lr: 0.0002, loss/train: 0.4372055787030998, loss/test: 0.6841402366946016, acc/train: 0.84542, acc/test: 0.7729\n",
      "epoch: 116, lr: 0.0002, loss/train: 0.4399572502621604, loss/test: 0.6896728567684753, acc/train: 0.84538, acc/test: 0.7749\n",
      "epoch: 117, lr: 0.0002, loss/train: 0.4374460170564749, loss/test: 0.6737255031549478, acc/train: 0.84538, acc/test: 0.7714\n",
      "epoch: 118, lr: 0.0002, loss/train: 0.43347453636586514, loss/test: 0.6775146334231654, acc/train: 0.8486, acc/test: 0.7767\n",
      "epoch: 119, lr: 0.0002, loss/train: 0.43684539922972776, loss/test: 0.6789516214328476, acc/train: 0.84656, acc/test: 0.774\n",
      "epoch: 120, lr: 0.0002, loss/train: 0.42910679885188635, loss/test: 0.6830673387533501, acc/train: 0.84946, acc/test: 0.7737\n",
      "epoch: 121, lr: 0.0002, loss/train: 0.4317496678103571, loss/test: 0.6923194199423247, acc/train: 0.84832, acc/test: 0.7718\n",
      "epoch: 122, lr: 0.0002, loss/train: 0.4290112577131032, loss/test: 0.6846280954306638, acc/train: 0.84966, acc/test: 0.7804\n",
      "epoch: 123, lr: 0.0002, loss/train: 0.4267949743953812, loss/test: 0.6834863737414155, acc/train: 0.84892, acc/test: 0.7779\n",
      "epoch: 124, lr: 0.0002, loss/train: 0.42439326613455475, loss/test: 0.6883446332774584, acc/train: 0.85074, acc/test: 0.7749\n",
      "epoch: 125, lr: 0.0002, loss/train: 0.42484856539827476, loss/test: 0.6897182883341101, acc/train: 0.84904, acc/test: 0.7776\n",
      "epoch: 126, lr: 0.0002, loss/train: 0.42284584331238056, loss/test: 0.6880972672112381, acc/train: 0.85084, acc/test: 0.7732\n",
      "epoch: 127, lr: 0.0002, loss/train: 0.42527536887799383, loss/test: 0.6902067495297782, acc/train: 0.85066, acc/test: 0.7763\n",
      "epoch: 128, lr: 0.0002, loss/train: 0.4196732279361057, loss/test: 0.6941249766681767, acc/train: 0.85312, acc/test: 0.7762\n",
      "epoch: 129, lr: 0.0002, loss/train: 0.41961890184665884, loss/test: 0.682974844039241, acc/train: 0.85128, acc/test: 0.7775\n",
      "epoch: 130, lr: 0.0002, loss/train: 0.4196445270801139, loss/test: 0.7039297045786169, acc/train: 0.85206, acc/test: 0.7727\n",
      "epoch: 131, lr: 0.0002, loss/train: 0.4160236396524302, loss/test: 0.7003435334072837, acc/train: 0.85292, acc/test: 0.77\n",
      "epoch: 132, lr: 0.0002, loss/train: 0.41527092830299417, loss/test: 0.6868110208571712, acc/train: 0.85176, acc/test: 0.7739\n",
      "epoch: 133, lr: 0.0002, loss/train: 0.4144310926461159, loss/test: 0.7039732272866406, acc/train: 0.85368, acc/test: 0.772\n",
      "epoch: 134, lr: 0.0002, loss/train: 0.41790640704772053, loss/test: 0.6871080236344398, acc/train: 0.8523, acc/test: 0.7785\n",
      "epoch: 135, lr: 0.0002, loss/train: 0.41506022474040155, loss/test: 0.691446035723143, acc/train: 0.85466, acc/test: 0.7764\n",
      "epoch: 136, lr: 0.0002, loss/train: 0.408999976027957, loss/test: 0.6996178570427473, acc/train: 0.85568, acc/test: 0.7763\n",
      "epoch: 137, lr: 0.0002, loss/train: 0.40950516887637967, loss/test: 0.6981643819356267, acc/train: 0.85458, acc/test: 0.774\n",
      "epoch: 138, lr: 0.0002, loss/train: 0.4097495715865089, loss/test: 0.7012047141413146, acc/train: 0.8551, acc/test: 0.7767\n",
      "epoch: 139, lr: 0.0002, loss/train: 0.41085715008818585, loss/test: 0.6962608600719066, acc/train: 0.85372, acc/test: 0.7733\n",
      "epoch: 140, lr: 0.0002, loss/train: 0.40676601204420904, loss/test: 0.695376040060309, acc/train: 0.8579, acc/test: 0.7761\n",
      "epoch: 141, lr: 0.0002, loss/train: 0.40797321082990795, loss/test: 0.7039034294931195, acc/train: 0.85592, acc/test: 0.7757\n",
      "epoch: 142, lr: 0.0002, loss/train: 0.4052083690453063, loss/test: 0.6924842792221263, acc/train: 0.85666, acc/test: 0.7743\n",
      "epoch: 143, lr: 0.0002, loss/train: 0.40452966475120894, loss/test: 0.7032239742671387, acc/train: 0.8574, acc/test: 0.7748\n",
      "epoch: 144, lr: 0.0002, loss/train: 0.4053789646652958, loss/test: 0.6953610370430765, acc/train: 0.85594, acc/test: 0.7756\n",
      "epoch: 145, lr: 0.0002, loss/train: 0.4061283049223673, loss/test: 0.7051881604556796, acc/train: 0.85576, acc/test: 0.7717\n",
      "epoch: 146, lr: 0.0002, loss/train: 0.40253526452557203, loss/test: 0.7139272648322431, acc/train: 0.85882, acc/test: 0.7722\n",
      "epoch: 147, lr: 0.0002, loss/train: 0.4018489526360846, loss/test: 0.6974453703512119, acc/train: 0.85766, acc/test: 0.7719\n",
      "epoch: 148, lr: 0.0002, loss/train: 0.3987363720565196, loss/test: 0.7083989902387692, acc/train: 0.85852, acc/test: 0.771\n",
      "epoch: 149, lr: 0.0002, loss/train: 0.3994093306762788, loss/test: 0.7056465258326712, acc/train: 0.8592, acc/test: 0.7739\n",
      "LR drop\n",
      "epoch: 150, lr: 2e-05, loss/train: 0.4016399309610772, loss/test: 0.6995108927352519, acc/train: 0.85784, acc/test: 0.772\n",
      "epoch: 151, lr: 2e-05, loss/train: 0.38916396260109093, loss/test: 0.7101865327810939, acc/train: 0.8634, acc/test: 0.7734\n",
      "epoch: 152, lr: 2e-05, loss/train: 0.38262812473127605, loss/test: 0.6899520022959649, acc/train: 0.86506, acc/test: 0.7739\n",
      "epoch: 153, lr: 2e-05, loss/train: 0.38345147078604347, loss/test: 0.6949473087546192, acc/train: 0.8639, acc/test: 0.7766\n",
      "epoch: 154, lr: 2e-05, loss/train: 0.38284617418523337, loss/test: 0.7002956474883647, acc/train: 0.8655, acc/test: 0.7756\n",
      "epoch: 155, lr: 2e-05, loss/train: 0.38061172734288606, loss/test: 0.7001522621021995, acc/train: 0.8665, acc/test: 0.7759\n",
      "epoch: 156, lr: 2e-05, loss/train: 0.3857698342608064, loss/test: 0.6932211612598805, acc/train: 0.86404, acc/test: 0.7799\n",
      "epoch: 157, lr: 2e-05, loss/train: 0.3802539602188808, loss/test: 0.6989889623998087, acc/train: 0.86538, acc/test: 0.7772\n",
      "epoch: 158, lr: 2e-05, loss/train: 0.37967250406589653, loss/test: 0.6930584179449685, acc/train: 0.86502, acc/test: 0.774\n",
      "epoch: 159, lr: 2e-05, loss/train: 0.3791209170809182, loss/test: 0.6950627312630038, acc/train: 0.86602, acc/test: 0.7758\n",
      "epoch: 160, lr: 2e-05, loss/train: 0.38164963777107963, loss/test: 0.7044756510589696, acc/train: 0.86486, acc/test: 0.7765\n",
      "epoch: 161, lr: 2e-05, loss/train: 0.37821983791830593, loss/test: 0.7132365186757679, acc/train: 0.8684, acc/test: 0.7739\n",
      "epoch: 162, lr: 2e-05, loss/train: 0.38006702949628807, loss/test: 0.7083120568643643, acc/train: 0.86556, acc/test: 0.7745\n",
      "epoch: 163, lr: 2e-05, loss/train: 0.37900212856814686, loss/test: 0.7093090252785743, acc/train: 0.86674, acc/test: 0.7724\n",
      "epoch: 164, lr: 2e-05, loss/train: 0.3820695170127522, loss/test: 0.6944484307041651, acc/train: 0.86518, acc/test: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329018a464d14575a1ac34fee80283a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='100.871 MB of 100.871 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc/test</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>acc/train</td><td>▁▃▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇██████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss/test</td><td>█▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss/train</td><td>█▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████████████████▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc/test</td><td>0.7812</td></tr><tr><td>acc/train</td><td>0.86518</td></tr><tr><td>epoch</td><td>164</td></tr><tr><td>loss/test</td><td>0.69445</td></tr><tr><td>loss/train</td><td>0.38207</td></tr><tr><td>lr</td><td>2e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-leaf-10</strong> at: <a href='https://wandb.ai/woj-jasinski/MRO-VIT/runs/18hlo7vz' target=\"_blank\">https://wandb.ai/woj-jasinski/MRO-VIT/runs/18hlo7vz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 47 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231129_184019-18hlo7vz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"MRO-VIT\")\n",
    "model = ViT(\n",
    "    img_size=32, \n",
    "    patch_size=4,    \n",
    "    in_channels=3, \n",
    "    embed_size=256, \n",
    "    num_heads=8, \n",
    "    depth=6,\n",
    "    n_classes=10\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
    "num_epochs = 165\n",
    "lr_drop_epochs = [100, 150]\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "progressbar = tqdm(total=num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        progressbar.set_description(f\"train loss: {loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progressbar.set_description(f\"test loss: {loss.item():.4f}\")\n",
    "\n",
    "    if epoch in lr_drop_epochs:\n",
    "        print(\"LR drop\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10\n",
    "\n",
    "    logs = {\n",
    "        \"epoch\": epoch,\n",
    "        \"lr\": optimizer.param_groups[0]['lr'], \n",
    "        \"loss/train\": train_loss / len(train_loader),\n",
    "        \"loss/test\": test_loss / len(test_loader),\n",
    "        \"acc/train\": train_correct / len(train_dataset),\n",
    "        \"acc/test\": test_correct / len(test_dataset),\n",
    "    }\n",
    "    wandb.log(logs)\n",
    "\n",
    "    if logs[\"loss/test\"] < best_test_loss:\n",
    "        best_test_loss = logs[\"loss/test\"]\n",
    "        model_path = f\"best_model_epoch_{epoch:03d}_test_loss{best_test_loss:.1e}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        wandb.save(model_path)\n",
    "\n",
    "        \n",
    "    progressbar.update(1)\n",
    "    print(\", \".join([f\"{k}: {v:}\" for k, v in logs.items()]))\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea878b-16a9-406a-b934-1ac55cc89443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
